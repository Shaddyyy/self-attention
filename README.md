# Self-Attention
The self-attention mechanism is the fundamental operation in the transformers model, which is the architechture behind the modern LLMs.

Here is the most useful doc out there, to understand it from basic to advanced backed by strong intuitions and some practical work.
https://peterbloem.nl/blog/transformers

This github repository includes single-head and multi-head self-attention, built in Pytorch from the very scratch. The workflow is also guided for better undestanding.

Additionally, I've attatched screen captures of both the attention mechanisms.
