{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0d043a",
   "metadata": {},
   "source": [
    "# Building single-head self-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a09303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a4b5a",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "### Any self-attention (single-head or multi-head) has learnable parameters(Wq, Wk, Wv), by which it is multiplied and forwarded. So it is just like a neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915c6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining single-head self-attention module\n",
    "\n",
    "class single_head_self_attentional(nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super.init()\n",
    "        self.k=k\n",
    "        \n",
    "        #Wq, Wk, Wv are just matrices which linearly scale the input vectors to form queries,keys,values\n",
    "        #using nn.Linear layer with no bias will do the job for you\n",
    "        #passing the i/p vectors through this layer will scale them\n",
    "        \n",
    "        self.toqueries=nn.Linear(self.k,self.k,bias=False) #Wq\n",
    "        self.tokeys=nn.Linear(self.k,self.k,bias=False)    #Wk\n",
    "        self.tovalues=nn.Linear(self.k,self.k,bias=False)  #Wv\n",
    "        \n",
    "    \n",
    "    def forward(self,X):\n",
    "        #X is an input torch tensor of the size (t * k) batch size is 1 for easy unbderstanding\n",
    "        #It represents an input sequence with t words and each word having the dimension k\n",
    "        \n",
    "        Q=self.toqueries(X)\n",
    "        K=self.tokeys(X)\n",
    "        V=self.tovalues(X)\n",
    "        \n",
    "        unscaled_weights=torch.mm(Q,K.transpose())\n",
    "        scaled_weights=unscaled_weights/(self.k ** (1/2)) #scaling for preventing large values going into softmax which \n",
    "                                            #will lead to derivate being near 0 and thus prevent/slow down learning\n",
    "            \n",
    "        \n",
    "        weights=F.softmax(scaled_weights,dim=1)\n",
    "        \n",
    "        result=torch.mm(weights,X)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641106dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "887c9f3d",
   "metadata": {},
   "source": [
    "# Building multi-head self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1d920",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "### Attention heads are the no. of times you want to perform self-attention on a single o/p vector.\n",
    "### 2 options\n",
    "### 1. Results are concatenated and then dimensions are reduced(higher time complexity)\n",
    "### 2. I/p vectors are scaled down, multi-head attention is performed, and the results are concatinated with additional scaling (lower time complexity)\n",
    "### Lets go with the 2nd option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7058dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining multi-head self-attention module\n",
    "\n",
    "class multi_head_self_attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,k,h=4):  \n",
    "        super.init()\n",
    "        \n",
    "        assert k%h==0  #then only the dimensions can be scaled \n",
    "                      #heads=4 by default as most dimensions are divisble by 4 \n",
    "            \n",
    "        self.k=k\n",
    "        self.heads=h\n",
    "        self.s=k//h #ineteger division\n",
    "        \n",
    "        \n",
    "        #small hack -> instead of applying several short linear layers(matrices) which are 3h in number (also, variable), we can just\n",
    "        #apply a single linear layer and split the results into chunks as required (which will be very easy)\n",
    "        \n",
    "        self.toqueries=nn.Linear(self.k,self.k,bias=False)\n",
    "        self.tokeys=nn.Linear(self.k,self.k,bias=False)\n",
    "        self.tovalues=nn.Linear(self.k,self.k,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,X):\n",
    "        #X is an input torch tensor of the size (t * k) batch size is 1 for easy unbderstanding\n",
    "        #It represents an input sequence with t words and each word having the dimension k\n",
    "        \n",
    "        t,h=X.size()\n",
    "        \n",
    "        Q=self.toqueries(X)\n",
    "        K=self.tokeys(X)\n",
    "        V=self.tovalues(X)\n",
    "        \n",
    "        #breaking them into small chunks/parts \n",
    "        \n",
    "        #original dimension was (t,k)\n",
    "        queries=Q.reshape(t,self.s,h) \n",
    "        keys=K.reshape(t,self.s,h)\n",
    "        values=V.reshape(t,self.s,h)\n",
    "        \n",
    "        #we need to do dot product between Q and K, and this is the same for all the heads\n",
    "        #so we make the head dimension as the batched dimension, so that the dot product is done for all the batches,\n",
    "        #thus for all the heads\n",
    "        #NOTE: dot product is always between 2 matrices, no matter if they are arranged in batches or not\n",
    "        \n",
    "        b_Q=queries.transpose(1,2).transpose(0,1)\n",
    "        b_K=keys.transpose(1,2).transpose(0,1)\n",
    "        b_V=values.transpose(1,2).transpose(0,1)\n",
    "        \n",
    "        #dimension of b_Q,b_K,b_V is (h,t,s)\n",
    "        \n",
    "        unscaled_weights=torch.bmm(b_Q,b_K.transpose(1,2))\n",
    "        scaled_weights=unscaled_weights/(self.s ** (1/2))\n",
    "        \n",
    "        weights=F.softmax(scaled_weights,dim=2)\n",
    "        \n",
    "        out=torch.bmm(weights,b_V)\n",
    "        #having dim (h,t,s)\n",
    "        \n",
    "        result=out.transpose(0,1).transpose(1,2).reshape(t,self.s*h)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f966d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
